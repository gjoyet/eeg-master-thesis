{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6U0luYry-sL",
        "outputId": "cbac2033-fe92-4e51-b99a-05f39a321d7f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7a47ed84aad0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from typing import Iterable\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import datetime\n",
        "\n",
        "import gdown\n",
        "\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "b5D2fEEKzEz5"
      },
      "outputs": [],
      "source": [
        "class ChronoNet(nn.Module):\n",
        "\n",
        "    def __init__(self, inception_dropout_p=0.0, gru_dropout_p=0.0):\n",
        "        super(ChronoNet, self).__init__()\n",
        "\n",
        "        self.idp = inception_dropout_p\n",
        "        self.gdp = gru_dropout_p\n",
        "\n",
        "        self.cnn_layers = nn.Sequential(\n",
        "            self.MultiscaleConv1D(64, 32),\n",
        "            #Â nn.ReLU(),\n",
        "            nn.Dropout(p=self.idp),\n",
        "            self.MultiscaleConv1D(96, 32),\n",
        "            # nn.ReLU(),\n",
        "            nn.Dropout(p=self.idp),\n",
        "            self.MultiscaleConv1D(96, 32),\n",
        "            # nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # ModuleList should contain 4 Sequential dropout-then-GRU containers\n",
        "        self.gru_layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Dropout(p=self.gdp),\n",
        "                nn.GRU(96, 32, batch_first=True)),\n",
        "            nn.Sequential(\n",
        "                nn.Dropout(p=self.gdp),\n",
        "                nn.GRU(32, 32, batch_first=True)),\n",
        "            nn.Sequential(\n",
        "                nn.Dropout(p=self.gdp),\n",
        "                nn.GRU(64, 32, batch_first=True)),\n",
        "            nn.Sequential(\n",
        "                nn.Dropout(p=self.gdp),\n",
        "                nn.GRU(96, 32, batch_first=True)),\n",
        "        ])\n",
        "\n",
        "        self.linear = nn.Linear(32, 1)\n",
        "\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, batch):\n",
        "        # Transpose back and forth because CNN modules expect time at last dimension instead of features.\n",
        "        batch = torch.transpose(batch, 1, 2)\n",
        "        cnn_out = self.cnn_layers(batch)\n",
        "        cnn_out = torch.transpose(cnn_out, 1, 2)\n",
        "\n",
        "        gru_out_0, _ = self.gru_layers[0](cnn_out)\n",
        "        gru_out_1, _ = self.gru_layers[1](gru_out_0)\n",
        "        gru_out_2, _ = self.gru_layers[2](torch.cat((gru_out_0, gru_out_1), dim=2))\n",
        "        gru_out_3, _ = self.gru_layers[3](torch.cat((gru_out_0, gru_out_1, gru_out_2), dim=2))\n",
        "\n",
        "        # maybe test concatenating with input\n",
        "        out = self.linear(gru_out_3)\n",
        "        score = self.sig(out)\n",
        "\n",
        "        return score\n",
        "\n",
        "    class MultiscaleConv1D(nn.Module):\n",
        "        def __init__(self, in_channels: int, out_channels: int, kernel_sizes: Iterable[int] = (2, 4, 8), stride: int = 1):\n",
        "            super(ChronoNet.MultiscaleConv1D, self).__init__()\n",
        "            # iterate the list and create a ModuleList of single Conv1d blocks\n",
        "            self.kernels = nn.ModuleList()\n",
        "            for k in kernel_sizes:\n",
        "                self.kernels.append(nn.Conv1d(in_channels, out_channels, k, stride=stride, padding=k//2 - 1))\n",
        "\n",
        "        def forward(self, batch):\n",
        "            # now you can build a single output from the list of convs\n",
        "            out = [module(batch) for module in self.kernels]\n",
        "            # concatenate at dim=1 since in convolutions features are at dim=1\n",
        "            return torch.cat(out, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lNej0PCMzJHG"
      },
      "outputs": [],
      "source": [
        "class CustomNPZDataset(Dataset):\n",
        "    def __init__(self, file_path):\n",
        "        # Load the .npz file in 'mmap_mode' for memory-efficient access\n",
        "        self.data = np.load(file_path, mmap_mode='r')\n",
        "\n",
        "        # Assume the .npz file contains two arrays: 'inputs' and 'labels'\n",
        "        self.inputs = self.data['epochs']\n",
        "        self.labels = self.data['labels']\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.inputs.shape[0]  # Return the number of samples (rows)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load a single input and label\n",
        "        input_data = self.inputs[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert to PyTorch tensors and return\n",
        "        return torch.tensor(input_data, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jD_sfu0L1xcj"
      },
      "outputs": [],
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "    elif isinstance(m, nn.LSTM):\n",
        "        for name, param in m.named_parameters():\n",
        "            if 'weight_ih' in name:\n",
        "                torch.nn.init.xavier_uniform_(param.data)\n",
        "            elif 'weight_hh' in name:\n",
        "                torch.nn.init.orthogonal_(param.data)\n",
        "            elif 'bias' in name:\n",
        "                param.data.fill_(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FokyfYc-zLJp",
        "outputId": "52db1247-e0ed-466a-b426-1ecdde002532"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'content/drive/My Drive/Colab Notebooks/training_data_200Hz_scaled_BALANCED.npz'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "balance = True\n",
        "scale = True  # if True, uses scaled data\n",
        "\n",
        "if balance:\n",
        "  file_id = '1BVrAZ5kg96Zqpwlfaea3WhGMDg8ovBU5'  # file containing balanced (and scaled) data\n",
        "elif scale:\n",
        "  file_id = '16CyXKsWCW4zkBM9CiSrleAoUi8gArZQm'  # file containing scaled data\n",
        "else:\n",
        "  file_id = '1ckbrLscgUmJHVR_yI4bdoSZyVEpgstD3'  # file containing unscaled data\n",
        "\n",
        "local_path = 'content/drive/My Drive/Colab Notebooks/'  # Replace with your desired local path\n",
        "\n",
        "gdown.download(\n",
        "    f'https://drive.google.com/uc?id={file_id}',\n",
        "    local_path,\n",
        "    quiet=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ix5xm3qxzLri"
      },
      "outputs": [],
      "source": [
        "if balance:\n",
        "  filename = os.path.join(local_path, 'training_data_200Hz_scaled_BALANCED.npz')\n",
        "elif scale:\n",
        "  filename = os.path.join(local_path, 'training_data_200Hz_scaled.npz')\n",
        "else:\n",
        "  filename = os.path.join(local_path, 'training_data_200Hz.npz')\n",
        "dataset = CustomNPZDataset(file_path=filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44VmzqmrzPdq",
        "outputId": "28c9a5ec-815a-4d80-bbc1-003d88de2d13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "# HYPERPARAMETERS\n",
        "downsample_factor = 5  # when 1 -> memory overload: can I save file in several steps?\n",
        "washout_factor = 900 / 2250  # 'time in ms you want to washout' / 'EEG window length in ms'\n",
        "learning_rate = 1e-4\n",
        "num_epochs = 15\n",
        "\n",
        "weight_decay = 1e-1\n",
        "inception_dropout_p = 0.5\n",
        "gru_dropout_p = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "K1CQuhm6zRwo"
      },
      "outputs": [],
      "source": [
        "# Split lengths (e.g., 80% train, 20% test)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)  # test num_workers = 1, 2, 4, ...\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Create model\n",
        "model = ChronoNet(inception_dropout_p=inception_dropout_p, gru_dropout_p=gru_dropout_p)\n",
        "model = model.to(device)\n",
        "model.apply(init_weights)\n",
        "\n",
        "loss_function = nn.BCELoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        },
        "id": "xoYAFNaYzTrB",
        "outputId": "1c9138b1-ed54-40a8-8255-7d1b88dcf94e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/15]:\n",
            "       Train Loss: 0.7018759\n",
            "  Validation Loss: 0.6957938\n",
            "     Elapsed Time:    17.63\n",
            "\n",
            "Epoch [2/15]:\n",
            "       Train Loss: 0.6942053\n",
            "  Validation Loss: 0.6949447\n",
            "     Elapsed Time:    17.92\n",
            "\n",
            "Epoch [3/15]:\n",
            "       Train Loss: 0.6911980\n",
            "  Validation Loss: 0.6972646\n",
            "     Elapsed Time:    17.69\n",
            "\n",
            "Epoch [4/15]:\n",
            "       Train Loss: 0.6892131\n",
            "  Validation Loss: 0.6964092\n",
            "     Elapsed Time:    17.92\n",
            "\n",
            "Epoch [5/15]:\n",
            "       Train Loss: 0.6871048\n",
            "  Validation Loss: 0.6955361\n",
            "     Elapsed Time:    17.88\n",
            "\n",
            "Epoch [6/15]:\n",
            "       Train Loss: 0.6850672\n",
            "  Validation Loss: 0.6960019\n",
            "     Elapsed Time:    18.19\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-165-e1c5545ba279>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m# inputs have shape (batch_size, sequence_length, num_features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_profile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDisableTorchFunctionSubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_function_exit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RecordFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    770\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_function_exit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_as_effectful_op_temporarily\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch_in_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fallthrough_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_dispatch_in_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfallthrough_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "epochs_train_loss = np.zeros(num_epochs)\n",
        "epochs_validation_loss = np.zeros(num_epochs)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    # training loop\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        # inputs have shape (batch_size, sequence_length, num_features)\n",
        "        model.zero_grad()\n",
        "\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        if i == 0:\n",
        "            outputs = model(inputs)\n",
        "            washout = int(outputs.shape[1] * washout_factor)\n",
        "\n",
        "        outputs = model(inputs)[:, washout:, :]\n",
        "\n",
        "        # reshape labels to match output\n",
        "        labels = labels.unsqueeze(-1).unsqueeze(-1).expand(-1, outputs.shape[1], -1)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        epochs_train_loss[epoch] += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epochs_train_loss[epoch] /= i+1\n",
        "\n",
        "    # validation loop\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "\n",
        "        for i, (inputs, labels) in enumerate(test_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)[:, washout:, :]\n",
        "\n",
        "            # reshape labels to match output\n",
        "            labels = labels.unsqueeze(-1).unsqueeze(-1).expand(-1, outputs.shape[1], -1)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            epochs_validation_loss[epoch] += loss.item()\n",
        "\n",
        "        epochs_validation_loss[epoch] /= i + 1\n",
        "        model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    print('Epoch [{}/{}]:\\n{:>17}: {:8.7f}\\n{:>17}: {:8.7f}\\n{:>17}: {:8.2f}\\n'.format(epoch + 1,\n",
        "                                                                                        num_epochs,\n",
        "                                                                                        'Train Loss',\n",
        "                                                                                        epochs_train_loss[epoch],\n",
        "                                                                                        'Validation Loss',\n",
        "                                                                                        epochs_validation_loss[\n",
        "                                                                                            epoch],\n",
        "                                                                                        'Elapsed Time',\n",
        "                                                                                        end - start))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nve1CV_SSzWk"
      },
      "outputs": [],
      "source": [
        "sns.set_context(\"paper\", font_scale=1.25)\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "sns.set_palette(sns.color_palette(\"deep\")[4::2])\n",
        "\n",
        "sns.lineplot(y=epochs_train_loss, x=range(1, num_epochs + 1), label='Training Loss')\n",
        "sns.lineplot(y=epochs_validation_loss, x=range(1, num_epochs + 1), label='Validation Loss')\n",
        "\n",
        "sns.despine()\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "# plt.title(\"Loss over epochs\")\n",
        "\n",
        "# TODO: save the data (both loss and accuracies) to be able to change the plot.\n",
        "np.save('chrononet_train_loss_{}ep_{}wd_{}idp_{}gdp.npy'.format(num_epochs, weight_decay, inception_dropout_p, gru_dropout_p), epochs_train_loss)\n",
        "np.save('chrononet_val_loss_{}ep_{}wd_{}idp_{}gdp.npy'.format(num_epochs, weight_decay, inception_dropout_p, gru_dropout_p), epochs_validation_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTyB_9JgFgmZ"
      },
      "outputs": [],
      "source": [
        "def plot_accuracies(data: np.ndarray = None, title: str = \"\",\n",
        "                    savefile: str = None, washout: int = None) -> None:\n",
        "    \"\"\"\n",
        "    Plots the mean accuracy over time with confidence band over subjects.\n",
        "    :param data: 2D numpy array, where each row is the decoding accuracy for one subject over all timesteps.\n",
        "    :param title: title of the plot.\n",
        "    :param savefile: file name to save the plot under. If None, no plot is saved.\n",
        "    :param washout:\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "\n",
        "    df = pd.DataFrame(data=data.T)\n",
        "    df = df.reset_index().rename(columns={'index': 'Time'})\n",
        "    df = df.melt(id_vars=['Time'], value_name='Mean_Accuracy', var_name='Subject')\n",
        "\n",
        "    sns.set_context(\"paper\", font_scale=1.25)\n",
        "\n",
        "    # Create a seaborn lineplot, passing the matrix directly to seaborn\n",
        "    plt.figure(figsize=(10, 6))  # Optional: Set the figure size\n",
        "\n",
        "    # Create the lineplot, seaborn will automatically calculate confidence intervals\n",
        "    sns.lineplot(data=df, x=(df['Time'] + washout) * 5 - 1000, y='Mean_Accuracy',\n",
        "                 errorbar='ci', label='Accuracy')  # BUT confidence band gets much larger with 'sd'\n",
        "    # Also, it is important to note that MVPA computes CIs over subjects, while the\n",
        "    # neural nets compute CIs over trials.Higher n makes for narrower CIs, i.e. neural\n",
        "    # nets will have much narrower CIs without this implying higher certainty.\n",
        "    sns.despine()\n",
        "\n",
        "    plt.axhline(y=0.5, color='orange', linestyle='dashdot', linewidth=1, label='Random Chance')\n",
        "    plt.axvline(x=0, ymin=0, ymax=0.05, color='black', linewidth=1, label='Stimulus Onset')\n",
        "\n",
        "    # Set plot labels and title\n",
        "    plt.xlabel('Time (ms)')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.title(title)\n",
        "\n",
        "    if savefile is not None:\n",
        "        plt.savefig('results/{}.png'.format(savefile))\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Idm_9_C3FjJk"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "\n",
        "    trainset_accuracies = torch.Tensor(0).to(device)\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)[:, washout:, :]\n",
        "\n",
        "        outputs[labels == 0] = 1 - outputs[labels == 0]  # invert scores if label is 0 (to represent accuracy)\n",
        "\n",
        "        trainset_accuracies = torch.cat((trainset_accuracies, outputs), dim=0)\n",
        "\n",
        "    testset_accuracies = torch.Tensor(0).to(device)\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)[:, washout:, :]\n",
        "\n",
        "        outputs[labels == 0] = 1 - outputs[labels == 0]  # invert scores if label is 0 (to represent accuracy)\n",
        "\n",
        "        testset_accuracies = torch.cat((testset_accuracies, outputs), dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3w96eGa0dkp"
      },
      "outputs": [],
      "source": [
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "\n",
        "        trainset_scores = torch.empty(0).to(device)\n",
        "        trainset_accuracies = torch.empty(0).to(device)\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)[:, washout:, :]\n",
        "\n",
        "            predictions = outputs >= 0.5\n",
        "            accuracy = predictions == labels.unsqueeze(-1).unsqueeze(-1).expand(-1, outputs.shape[1], -1)\n",
        "\n",
        "            outputs[labels == 0] = 1 - outputs[labels == 0]  # invert scores if label is 0 (to represent accuracy)\n",
        "\n",
        "            trainset_accuracies = torch.cat((trainset_accuracies, accuracy), dim=0)\n",
        "            trainset_scores = torch.cat((trainset_scores, outputs), dim=0)\n",
        "\n",
        "        testset_scores = torch.empty(0).to(device)\n",
        "        testset_accuracies = torch.empty(0).to(device)\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)[:, washout:, :]\n",
        "\n",
        "            predictions = outputs >= 0.5\n",
        "            accuracy = predictions == labels.unsqueeze(-1).unsqueeze(-1).expand(-1, outputs.shape[1], -1)\n",
        "\n",
        "            outputs[labels == 0] = 1 - outputs[labels == 0]  # invert scores if label is 0 (to represent accuracy)\n",
        "\n",
        "            testset_accuracies = torch.cat((testset_accuracies, accuracy), dim=0)\n",
        "            testset_scores = torch.cat((testset_scores, outputs), dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CS-b77AEM5C1"
      },
      "outputs": [],
      "source": [
        "# plot_accuracies(data=trainset_scores.squeeze().cpu().numpy(), title='Training Scores', savefile=None, washout=washout)\n",
        "\n",
        "# plot_accuracies(data=testset_scores.squeeze().cpu().numpy(), title='Validation Scores', savefile=None, washout=washout)\n",
        "\n",
        "np.save('chrononet_train_sco_{}ep_{}wd_{}idp_{}gdp.npy'.format(num_epochs, weight_decay, inception_dropout_p, gru_dropout_p),\n",
        "        trainset_scores.squeeze().cpu().numpy())\n",
        "np.save('chrononet_val_sco_{}ep_{}wd_{}idp_{}gdp.npy'.format(num_epochs, weight_decay, inception_dropout_p, gru_dropout_p),\n",
        "        testset_scores.squeeze().cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwxOjkrw0pMS"
      },
      "outputs": [],
      "source": [
        "# TODO: adapt axes of plot (with downsample factor / washout or in method directly)\n",
        "# plot_accuracies(data=trainset_accuracies.squeeze().cpu().numpy(), title='Training Accuracy', savefile=None, washout=washout)\n",
        "\n",
        "# plot_accuracies(data=testset_accuracies.squeeze().cpu().numpy(), title='Validation Accuracy', savefile=None, washout=washout)\n",
        "\n",
        "# TODO: save accuracies as data\n",
        "np.save('chrononet_train_acc_{}ep_{}wd_{}idp_{}gdp.npy'.format(num_epochs, weight_decay, inception_dropout_p, gru_dropout_p),\n",
        "        trainset_accuracies.squeeze().cpu().numpy())\n",
        "np.save('chrononet_val_acc_{}ep_{}wd_{}idp_{}gdp.npy'.format(num_epochs, weight_decay, inception_dropout_p, gru_dropout_p),\n",
        "        testset_accuracies.squeeze().cpu().numpy())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}